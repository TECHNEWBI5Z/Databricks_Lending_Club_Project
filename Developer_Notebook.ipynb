{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "### 1. Select the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs ls 'dbfs:/databricks-datasets/lending-club-loan-stats/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.head(\"dbfs:/databricks-datasets/lending-club-loan-stats/lendingClubData.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "### 2. Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_df = spark.read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"dbfs:/databricks-datasets/lending-club-loan-stats/LoanStats_2018Q2.csv\")\n",
    "\n",
    "display(lending_club_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_club_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "### 3. Create member_Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----use SHA-2 DSA FOR TO GENERATE UNIQUE MEMBER_ID\n",
    "subset = lending_club_df['emp_title', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'grade', 'term', 'zip_code' ]\n",
    "\n",
    "from pyspark.sql.functions import sha2, concat_ws\n",
    "\n",
    "LCD_1 = lending_club_df.withColumn(\"member_id\", sha2(concat_ws(\"||\", * subset), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(LCD_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "### 4. check the duplicate member_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "display(LCD_1.select(\"member_id\").groupBy(\"member_id\").count().alias(\"count\").where(col(\"count\") > 1).orderBy(\"count\", ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "### 5. get rid of biased member_id records \n",
    "##### i.e., dublicate member_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "display(LCD_1.select(\"*\").where(col(\"member_id\") == \"e9891d14a32ca8b2160b06c6d57ecff9cd66dcfa205fedab9ab9a4af1587a88e\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a temporary DataFrame with member_id and their counts\n",
    "LCD_TEMP = LCD_1.groupBy(\"member_id\").count().withColumnRenamed(\"count\", \"member_count\").where(col(\"member_count\") > 1)\n",
    "\n",
    "# Collect the member_ids with more than one occurrence into a list\n",
    "member_ids_to_exclude = [row.member_id for row in LCD_TEMP.select(\"member_id\").collect()]\n",
    "\n",
    "# Filter the original DataFrame to exclude those member_ids\n",
    "LCD_2 = LCD_1.filter(~col(\"member_id\").isin(member_ids_to_exclude))\n",
    "\n",
    "# Display the result\n",
    "display(LCD_2)\n",
    "\n",
    "#create temp table\n",
    "LCD_2.createOrReplaceTempView(\"LCD_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCD_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "### 6. Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. borrower_info_df data\n",
    "borrower_info_df = LCD_2.select(['member_id', 'emp_title', 'emp_length', 'home_ownership', \n",
    "                              'annual_inc', 'verification_status', 'zip_code', 'addr_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. loan_details_df data\n",
    "loan_details_df = LCD_2.select([ 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', \n",
    "                             'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'issue_d', \n",
    "                             'loan_status', 'pymnt_plan', 'purpose', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Payment and Recovery Dataset\n",
    "payment_recovery_df = LCD_2.select(['member_id', 'total_pymnt', 'total_pymnt_inv', \n",
    "                                 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', \n",
    "                                 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', \n",
    "                                 'last_pymnt_amnt', 'next_pymnt_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Credit and Risk Dataset\n",
    "credit_risk_df = LCD_2.select(['member_id', 'dti', 'delinq_2yrs', 'earliest_cr_line', \n",
    "                            'inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record', \n",
    "                            'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', \n",
    "                            'initial_list_status', 'out_prncp', 'out_prncp_inv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 7. Write back the segregated datasets into staging layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataframe into staging layer of dbfs\n",
    "dbutils.fs.mkdirs(\"/lending_club_project/staging/\")\n",
    "\n",
    "borrower_info_df.repartition(1).write.format(\"parquet\").mode(\"overwrite\").option(\"header\", \"true\").option(\"path\", \"/lending_club_project/staging/borrower_info/\").save()\n",
    "\n",
    "loan_details_df.repartition(1).write.format(\"parquet\").mode(\"overwrite\").option(\"header\", \"true\").option(\"path\", \"/lending_club_project/staging/loan_details/\").save()\n",
    "\n",
    "payment_recovery_df.repartition(1).write.format(\"parquet\").mode(\"overwrite\").option(\"header\", \"true\").option(\"path\", \"/lending_club_project/staging/payment_recovery/\").save()\n",
    "\n",
    "credit_risk_df.repartition(1).write.format(\"parquet\").mode(\"overwrite\").option(\"header\", \"true\").option(\"path\", \"/lending_club_project/staging/credit_risk/\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 1.Cleaning of borrower_details_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### 1a. define schema and assign column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = loan_info_df.columns\n",
    "print(columns)\n",
    "schema = 'member_id string, emp_title string , emp_length string, home_ownership string, annual_inc double, verification_status string, zip_code string, addr_state string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_info_df = spark.read.schema(schema).parquet(\"dbfs:/lending_club_project/staging/borrower_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### 1b. add ingestion date and country column default as USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "dfa1 = loan_info_df.withColumn(\"ingestion_date\", current_timestamp()).withColumn(\"country\", lit(\"USA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 1C. remove records where annual_inc is zero\n",
    "####### B/C they are not eligible for credit loan\n",
    "####### dfa1.where(col(\"annual_inc\") == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa2 = dfa1.where(col(\"annual_inc\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### 1d. convert 'null' employee title into others and convert the case into sentence case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a Python function to convert text to sentence case\n",
    "def to_sentence_case(text):\n",
    "    if text:\n",
    "        return text.title()\n",
    "    return None\n",
    "\n",
    "# Register the function as a UDF\n",
    "to_sentence_case_udf = udf(to_sentence_case, StringType())\n",
    "\n",
    "dfa2 = dfa2.withColumn(\"emp_title\", to_sentence_case_udf(dfa2[\"emp_title\"]))\n",
    "\n",
    "dfa3 = dfa2.fillna({\"emp_title\": \"Others\"})\n",
    "\n",
    "display(dfa3.select(\"emp_title\").groupBy(\"emp_title\").count().alias(\"count\").orderBy(\"count\", ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### 1e.Convert employee length into integer and revert n/a into 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, avg, when\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Assuming `spark` is your SparkSession and `dfa3` is your DataFrame\n",
    "\n",
    "# Clean `emp_length` column using regexp_replace to keep only numeric characters\n",
    "dfa3_cleaned = dfa3.withColumn(\"emp_length\", regexp_replace(col(\"emp_length\"), \"[^0-9]\", \"\"))\n",
    "\n",
    "# Calculate average of emp_length (assuming it's numeric)\n",
    "avg_emp_length = dfa3_cleaned.select(avg(col(\"emp_length\"))).collect()[0][0]\n",
    "\n",
    "# Replace blank cells with the calculated average and handle type casting\n",
    "dfa4 = dfa3_cleaned.withColumn(\"emp_length\", when(col(\"emp_length\") == \"\", avg_emp_length).otherwise(col(\"emp_length\"))) \\\n",
    "                           .withColumn(\"emp_length\", col(\"emp_length\").cast(\"int\")) \\\n",
    "                           .withColumnRenamed(\"emp_length\", \"emp_length_in_years\") \\\n",
    "                           .fillna({\"emp_length_in_years\": 6})  # Assuming you want to fill any remaining nulls\n",
    "\n",
    "# Display the DataFrame\n",
    "display(dfa4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 1f. Verify any anomaly present in zip_code column and address_state column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, col\n",
    "\n",
    "# Assuming `df` is your DataFrame and `zip_code` is the column containing zip codes\n",
    "zip_veri = dfa4.filter(length(col(\"zip_code\")) > 5).count()\n",
    "\n",
    "# Display or further process the filtered DataFrame\n",
    "print(filtered_df)\n",
    "\n",
    "# Assuming `df` is your DataFrame and `zip_code` is the column containing zip codes\n",
    "addr_state_df = dfa4.filter(length(col(\"addr_state\")) > 2).count()\n",
    "\n",
    "# Display or further process the filtered DataFrame\n",
    "print(addr_state_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 2. Cleaning of loan_details_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### 2a. define schema and assign column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lend_info_df = spark.read.option(\"inferschema\", \"true\").parquet(\"/lending_club_project/staging/loan_details/\")\n",
    "display(lend_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 2b. Convert term column into tenure_period_in_month and cast it into integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "dfb1 = lend_info_df.withColumn(\"term\", regexp_replace(col(\"term\"), \"[^0-9]\", \"\")).withColumn(\"term\", col(\"term\").cast(\"int\")).withColumnRenamed(\"term\", \"tenure_period_in_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 2c. Convert int_rate into interest_rate_in_percentage and cast it into double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the code to remove non-numeric characters and cast the column to double\n",
    "dfb2 = dfb1.withColumn(\"int_rate\", regexp_replace(col(\"int_rate\"), \"[^0-9.]\", \"\").cast(\"double\")) \\\n",
    "           .withColumnRenamed(\"int_rate\", \"interest_rate_in_percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### 2d. Convert issue_d into issued_date into date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, concat, lit, to_date, col\n",
    "\n",
    "# Convert 'issue_d' from string to date format\n",
    "dfb3 = dfb2.withColumn(\"issue_d\", to_date(col(\"issue_d\"), \"MMM-yy\")).withColumnRenamed(\"issue_d\",\"issued_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 2e. Cast purpose column into sentence case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a Python function to convert text to sentence case\n",
    "def to_sentence_case(text):\n",
    "    if text:\n",
    "        return text.title()\n",
    "    return None\n",
    "\n",
    "# Register the UDF\n",
    "to_sent_case_udf = udf(to_sentence_case, StringType())\n",
    "\n",
    "# Apply the UDF to the 'purpose' column\n",
    "dfb4 = dfb3.withColumn(\"purpose\", to_sent_case_udf(col(\"purpose\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 2f. Drop unnecessery Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb5 = dfb4.drop(\"pymnt_plan\", \"title\")\n",
    "display(dfb5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "## 3. cleaning of payment_recovery_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### 3a. define schema and assign column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_recovery_df = spark.read.option(\"inferSchema\", \"true\").parquet(\"/lending_club_project/staging/payment_recovery/\")\n",
    "display(payment_recovery_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### Relationships:\n",
    "I. total_pymnt = total_rec_prncp + total_rec_int + other_fees:\n",
    "\n",
    "The total payments made (total_pymnt) are the sum of the principal repaid (total_rec_prncp), the interest paid (total_rec_int), and any other fees (e.g., late fees).\n",
    "\n",
    "II. total_pymnt and total_pymnt_inv:\n",
    "These should be closely related because the total payments made by the borrower (total_pymnt) should largely correspond to the payments received by the investors (total_pymnt_inv).\n",
    "Differences may arise due to service fees deducted from the payments before they are passed on to investors.\n",
    "\n",
    "III. total_rec_prncp and total_rec_int:\n",
    "\n",
    "These two components together make up the bulk of total_pymnt.\n",
    "Over time, as more of the loan principal is paid down, total_rec_prncp will increase.\n",
    "The interest paid (total_rec_int) depends on the interest rate and the remaining principal balance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### 3b. Cast last_pymnt_d in to date and rename col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "dfc1 = payment_recovery_df.withColumn(\"last_pymnt_d\", to_date(col(\"last_pymnt_d\"), \"MMM-yy\")).withColumnRenamed(\"last_pymnt_d\", \"last_payment_date\")\n",
    "dfc2 = dfc1.withColumn(\"next_pymnt_d\", to_date(col(\"next_pymnt_d\"), \"MMM-yy\")).withColumnRenamed(\"next_pymnt_d\", \"next_payment_date\")\n",
    "display(dfc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 4. Cleaning of Credit risk dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### 4a. define schema and assign column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_risk_df = spark.read.option(\"inferSchema\", \"true\").parquet(\"/lending_club_project/staging/credit_risk/\")\n",
    "display(credit_risk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### Details to know\n",
    "1. Personal and Loan Information: member_id uniquely identifies borrowers.\n",
    "2. Financial Ratios: dti shows debt burden relative to income.\n",
    "3. Delinquency Metrics: delinq_2yrs and mths_since_last_delinq track past due payments and recency.\n",
    "4. Credit History: earliest_cr_line and inq_last_6mths show the length of credit history and recent inquiries.\n",
    "5. Public Records: mths_since_last_record and pub_rec indicate public derogatory records.\n",
    "6. Current Accounts: open_acc and total_acc show current and total credit accounts.\n",
    "7. Revolving Credit: revol_bal and revol_util measure revolving credit balance and usage.\n",
    "8. Loan Status: initial_list_status provides the initial status of loan listings.\n",
    "9. Principal Balances: out_prncp and out_prncp_inv show outstanding principal balances from borrower and investor perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### 4a. Filling Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, length, when, regexp_replace, to_date, mean, stddev\n",
    "\n",
    "dfd1 = credit_risk_df.fillna({\n",
    "    'dti': credit_risk_df.select(mean(col('dti'))).collect()[0][0],  # Fill with mean\n",
    "    'delinq_2yrs': 0,  # Fill with 0 if no delinquency\n",
    "    'inq_last_6mths': 0,\n",
    "    'mths_since_last_delinq': -1,  # Placeholder for missing values\n",
    "    'mths_since_last_record': -1,\n",
    "    'pub_rec': 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 4b. conversion of date earliest_cr_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type conversion\n",
    "dfd2 = dfd1.withColumn(\"earliest_cr_line\", to_date(col(\"earliest_cr_line\"), 'MM/yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 4c. compare both mean and standard deveviation are in line or not\n",
    "####### results found closer value data seems to be in line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti_mean = dfd2.select(mean(col('dti'))).collect()[0][0]\n",
    "print(dti_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti_stddev = dfd2.select(stddev(col('dti'))).collect()[0][0]\n",
    "print(dti_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 4d. Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates\n",
    "dfd3 = dfd2.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### 4e. cast revol_util into double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd4 = dfd3.withColumn(\"revol_util\", regexp_replace(col(\"revol_util\"), \"[^0-9.]\", \"\")).withColumnRenamed(\"revol_util\",\"revol_util_in_percentage\").drop(\"mths_since_last_record\").withColumn(\"revol_util_in_percentage\", col(\"revol_util_in_percentage\").cast(\"float\"))\n",
    "display(dfd4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 5. ultra cleaned Credit risk dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 5a. exclude the missing value i.e -1 from mths_since_last_delinq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd5 = dfd4.where(col(\"mths_since_last_delinq\") != -1)\n",
    "display(dfd5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dfd5.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 6. Write back datasets into delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/lending_club_project/landing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa4.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/lending_club_project/landing/customers_details_datasets.delta\") \\\n",
    "    .saveAsTable(\"customers_details_datasets\")\n",
    "\n",
    "dfb5.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/lending_club_project/landing/loan_details_datasets.delta\") \\\n",
    "    .saveAsTable(\"loan_details_datasets\")\n",
    "\n",
    "dfc2.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/lending_club_project/landing/payment_recovery_datasets.delta\") \\\n",
    "    .saveAsTable(\"payment_recovery_datasets\")\n",
    "\n",
    "dfd4.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/lending_club_project/landing/credit_risk_dataset.delta\") \\\n",
    "    .saveAsTable(\"credit_risk_dataset\")\n",
    "\n",
    "dfd5.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/lending_club_project/landing/ultra_clean_credit_risk_dataset.delta\") \\\n",
    "    .saveAsTable(\"ultra_clean_credit_risk_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.joinReorder.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 7. Read delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lending_club_crunched_df = spark.sql(\"\"\"SELECT \n",
    "    U.member_id,\n",
    "    C.emp_title, \n",
    "    C.emp_length_in_years, \n",
    "    C.home_ownership, \n",
    "    C.annual_inc, \n",
    "    C.verification_status, \n",
    "    C.zip_code, \n",
    "    C.addr_state, \n",
    "    C.ingestion_date, \n",
    "    C.country,\n",
    "    \n",
    "    L.loan_amnt, \n",
    "    L.funded_amnt, \n",
    "    L.funded_amnt_inv, \n",
    "    L.tenure_period_in_month, \n",
    "    L.interest_rate_in_percentage, \n",
    "    L.installment, \n",
    "    L.grade, \n",
    "    L.sub_grade, \n",
    "    L.issued_date, \n",
    "    L.loan_status, \n",
    "    L.purpose,\n",
    "    \n",
    "    P.total_pymnt, \n",
    "    P.total_pymnt_inv, \n",
    "    P.total_rec_prncp, \n",
    "    P.total_rec_int, \n",
    "    P.total_rec_late_fee, \n",
    "    P.recoveries, \n",
    "    P.collection_recovery_fee, \n",
    "    P.last_payment_date, \n",
    "    P.last_pymnt_amnt, \n",
    "    P.next_payment_date,\n",
    "    \n",
    "    U.dti, \n",
    "    U.delinq_2yrs, \n",
    "    U.earliest_cr_line, \n",
    "    U.inq_last_6mths, \n",
    "    U.mths_since_last_delinq, \n",
    "    U.open_acc, \n",
    "    U.pub_rec, \n",
    "    U.revol_bal, \n",
    "    U.revol_util_in_percentage, \n",
    "    U.total_acc, \n",
    "    U.initial_list_status, \n",
    "    U.out_prncp, \n",
    "    U.out_prncp_inv\n",
    "\n",
    "FROM ultra_clean_credit_risk_dataset U\n",
    "LEFT JOIN customers_details_datasets C ON U.member_id = C.member_id\n",
    "LEFT JOIN loan_details_datasets L ON U.member_id = L.member_id\n",
    "LEFT JOIN payment_recovery_datasets P ON U.member_id = P.member_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalised total cleaned records\n",
    "display(lending_club_crunched_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lending_club_crunched_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/lending_club_project/landing/lending_club_crunched_df.delta\") \\\n",
    "    .saveAsTable(\"lending_club_crunched_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "### BUSINESS USE CASE\n",
    "##### 8. Calculating Credit Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "###### To calculate a credit score using a traditional approach without machine learning, you can use a scoring model based on weighted factors. Each factor is assigned a weight based on its importance in determining creditworthiness. Here's how you can do it:\n",
    "\n",
    "1. Determine the Factors: Identify the key factors that will influence the credit score.\n",
    "2. Assign Weights: Assign weights to each factor based on its importance.\n",
    "3. Calculate Scores: Normalize and calculate the score for each factor.\n",
    "4. Compute Final Credit Score: Aggregate the scores based on their weights.\n",
    "\n",
    "Example Credit Score Calculation\n",
    "Let's use the following factors:\n",
    "\n",
    "Debt-to-Income Ratio (dti)\n",
    "Number of Delinquencies in the Last 2 Years (delinq_2yrs)\n",
    "Revolving Balance (revol_bal)\n",
    "Revolving Utilization (revol_util_in_percentage)\n",
    "Number of Open Accounts (open_acc)\n",
    "Total Accounts (total_acc)\n",
    "Outstanding Principal (out_prncp)\n",
    "Outstanding Principal (Investor) (out_prncp_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF1 = spark.read.format(\"delta\").option(\"inferSchema\", \"true\").load(\"dbfs:/lending_club_project/landing/lending_club_crunched_dataset.delta\")\n",
    "display(DF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md \n",
    "#### 8b.Drop rows with missing values in critical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2 = DF1.dropna(subset=[\"member_id\", \"loan_amnt\", \"funded_amnt\", \"annual_inc\", \"dti\", \"delinq_2yrs\", \"revol_bal\", \"total_acc\", \"out_prncp\", \"out_prncp_inv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 8c.Filter out rows with revol_util_in_percentage > 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF3 = DF2.filter(col(\"revol_util_in_percentage\") <= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 8d.Impute missing values in non-critical columns with the median or a placeholder value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "median_annual_inc = DF3.approxQuantile(\"annual_inc\", [0.5], 0.0)[0]\n",
    "DF4 = DF3.withColumn(\"annual_inc\", when(col(\"annual_inc\").isNull(), median_annual_inc).otherwise(col(\"annual_inc\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 8e. Normalize and calculate scores for each factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF4 = DF4.withColumn(\"income_score\", when(col(\"annual_inc\") > 100000, 100)\n",
    "                                    .when(col(\"annual_inc\") > 75000, 75)\n",
    "                                    .when(col(\"annual_inc\") > 50000, 50)\n",
    "                                    .when(col(\"annual_inc\") > 25000, 25)\n",
    "                                    .otherwise(0))\n",
    "\n",
    "DF4 = DF4.withColumn(\"dti_score\", when(col(\"dti\") < 10, 100)\n",
    "                                  .when(col(\"dti\") < 20, 75)\n",
    "                                  .when(col(\"dti\") < 30, 50)\n",
    "                                  .otherwise(0))\n",
    "\n",
    "DF4 = DF4.withColumn(\"delinq_score\", when(col(\"delinq_2yrs\") == 0, 100)\n",
    "                                     .when(col(\"delinq_2yrs\") == 1, 50)\n",
    "                                     .otherwise(0))\n",
    "\n",
    "DF4 = DF4.withColumn(\"revol_bal_score\", when(col(\"revol_bal\") < 5000, 100)\n",
    "                                      .when(col(\"revol_bal\") < 10000, 75)\n",
    "                                      .when(col(\"revol_bal\") < 20000, 50)\n",
    "                                      .otherwise(0))\n",
    "\n",
    "DF4 = DF4.withColumn(\"revol_util_score\", when(col(\"revol_util_in_percentage\") < 30, 100)\n",
    "                                       .when(col(\"revol_util_in_percentage\") < 50, 75)\n",
    "                                       .when(col(\"revol_util_in_percentage\") < 70, 50)\n",
    "                                       .otherwise(0))\n",
    "\n",
    "DF4 = DF4.withColumn(\"total_acc_score\", when(col(\"total_acc\") > 20, 100)\n",
    "                                      .when(col(\"total_acc\") > 10, 75)\n",
    "                                      .when(col(\"total_acc\") > 5, 50)\n",
    "                                      .otherwise(0))\n",
    "\n",
    "DF4 = DF4.withColumn(\"out_prncp_score\", when(col(\"out_prncp\") == 0, 100)\n",
    "                                      .when(col(\"out_prncp\") < 1000, 75)\n",
    "                                      .when(col(\"out_prncp\") < 5000, 50)\n",
    "                                      .otherwise(0))\n",
    "\n",
    "DF4 = DF4.withColumn(\"out_prncp_inv_score\", when(col(\"out_prncp_inv\") == 0, 100)\n",
    "                                          .when(col(\"out_prncp_inv\") < 1000, 75)\n",
    "                                          .when(col(\"out_prncp_inv\") < 5000, 50)\n",
    "                                          .otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 8f.Aggregate Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF5 = DF4.withColumn(\"total_score\", col(\"income_score\") + col(\"dti_score\") + col(\"delinq_score\") + col(\"revol_bal_score\") + col(\"revol_util_score\") + col(\"total_acc_score\") + col(\"out_prncp_score\") + col(\"out_prncp_inv_score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 8g. Normalize the Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 800  # Maximum possible score based on our criteria\n",
    "min_score = 300  # Minimum possible score to match typical credit score ranges\n",
    "DF6 = DF5.withColumn(\"credit_score\", (col(\"total_score\") / max_score) * (850 - 300) + 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 8h. Grading the Credit Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF7 = DF6.withColumn(\"credit_grade\", when(col(\"credit_score\") >= 750, \"very_good\")\n",
    "                                    .when(col(\"credit_score\") >= 700, \"good\")\n",
    "                                    .when(col(\"credit_score\") >= 650, \"medium\")\n",
    "                                    .when(col(\"credit_score\") >= 600, \"bad\")\n",
    "                                    .otherwise(\"very_bad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_good_df = DF7.where(col(\"credit_grade\")== 'very_good')\n",
    "good_df = DF7.where(col(\"credit_grade\")== 'good')\n",
    "medium_df = DF7.where(col(\"credit_grade\")== 'medium')\n",
    "bad_df = DF7.where(col(\"credit_grade\")== 'bad')\n",
    "very_bad_df = DF7.where(col(\"credit_grade\")== 'very_bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 9. write back the graded finalised results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/lending_club_project/reporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_good_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/lending_club_project/reporting/very_good_score_dataset.delta\") \\\n",
    "    .saveAsTable(\"very_good_score_dataset\")\n",
    "\n",
    "good_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/lending_club_project/reporting/good_score_dataset.delta\") \\\n",
    "    .saveAsTable(\"good_score_dataset\")\n",
    "\n",
    "medium_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/lending_club_project/reporting/medium_score_dataset.delta\") \\\n",
    "    .saveAsTable(\"medium_score_dataset\")\n",
    "\n",
    "bad_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/lending_club_project/reporting/bad_score_dataset.delta\") \\\n",
    "    .saveAsTable(\"bad_score_dataset\")\n",
    "\n",
    "very_bad_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/lending_club_project/reporting/very_bad_score_dataset.delta\") \\\n",
    "    .saveAsTable(\"very_bad_score_dataset\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
